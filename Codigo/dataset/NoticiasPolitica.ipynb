{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: couchbase in c:\\program files\\python313\\lib\\site-packages (4.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install couchbase\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from couchbase.cluster import Cluster\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.options import ClusterOptions, ClusterTimeoutOptions\n",
    "from couchbase.exceptions import CouchbaseException\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de Couchbase\n",
    "endpoint = \"couchbases://cb.rwijg3tkoqodgahr.cloud.couchbase.com\"\n",
    "username = \"BDD_POLITICA\"\n",
    "password = \"Admin_1234\"  # Reemplazar con la contraseña real\n",
    "bucket_name = \"BDD_POLITICA\"\n",
    "scope_name = \"OpinionPublica\"\n",
    "collection_name = \"NoticiasPoliticas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectado correctamente a la colección.\n"
     ]
    }
   ],
   "source": [
    "# Couchbase connection setup\n",
    "auth = PasswordAuthenticator(username, password)\n",
    "timeout_options = ClusterTimeoutOptions(kv_timeout=timedelta(seconds=10))\n",
    "options = ClusterOptions(auth, timeout_options=timeout_options)\n",
    "\n",
    "# Conexión al cluster\n",
    "cluster = Cluster(endpoint, options)\n",
    "cluster.wait_until_ready(timedelta(seconds=10))\n",
    "\n",
    "# Acceso al bucket, scope y colección\n",
    "cb = cluster.bucket(bucket_name)\n",
    "cb_coll = cb.scope(scope_name).collection(collection_name)\n",
    "\n",
    "# Confirmación\n",
    "print(\"Conectado correctamente a la colección.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://cnnespanol.cnn.com/latinoamerica/ecuador/\"\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "response = requests.get(base_url, headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la página de noticias políticas en El Telégrafo\n",
    "url = \"https://www.eltelegrafo.com.ec/politica\"\n",
    "\n",
    "# Obtener la respuesta del servidor\n",
    "response = requests.get(url)\n",
    "\n",
    "# Si la respuesta es exitosa, analizamos el contenido con BeautifulSoup\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceso exitoso a la página principal (https://www.teleamazonas.com/)\n",
      "Datos guardados en 'noticias_teleamazonas_principal.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página principal de Teleamazonas\n",
    "url = 'https://www.teleamazonas.com/'\n",
    "\n",
    "# Lista para almacenar las noticias\n",
    "noticias = []\n",
    "\n",
    "# Hacer la solicitud GET para obtener el contenido de la página\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"Acceso exitoso a la página principal ({url})\")\n",
    "    \n",
    "    # Crear el objeto BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar todos los títulos de noticias con la clase 'entry-title'\n",
    "    articulos = soup.find_all('h3', class_='entry-title')\n",
    "    \n",
    "    # Extraer el título y el enlace de cada artículo\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)  # Extraer el texto del título\n",
    "        enlace = articulo.find('a')['href'] if articulo.find('a') else None  # Extraer el enlace\n",
    "        \n",
    "        if enlace:\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': enlace\n",
    "            })\n",
    "else:\n",
    "    print(f\"Error al acceder a la página principal. Código de estado: {response.status_code}\")\n",
    "\n",
    "# Convertir las noticias extraídas en un DataFrame de pandas\n",
    "df_noticias = pd.DataFrame(noticias)\n",
    "\n",
    "# Guardar el DataFrame en un archivo JSON\n",
    "df_noticias.to_json('noticias_teleamazonas_principal.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "print(\"Datos guardados en 'noticias_teleamazonas_principal.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceso exitoso a la página principal (https://www.eluniverso.com/)\n",
      "Datos guardados en 'noticias_eluniverso_principal.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página principal de El Universo\n",
    "url = 'https://www.eluniverso.com/'\n",
    "\n",
    "# Lista para almacenar las noticias\n",
    "noticias = []\n",
    "\n",
    "# Hacer la solicitud GET para obtener el contenido de la página\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"Acceso exitoso a la página principal ({url})\")\n",
    "    \n",
    "    # Crear el objeto BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar todos los títulos de noticias con la clase 'text-base m-0 font-bold font-primary'\n",
    "    articulos = soup.find_all('h2', class_='text-base m-0 font-bold font-primary')\n",
    "    \n",
    "    # Extraer el título y el enlace de cada artículo\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)  # Extraer el texto del título\n",
    "        enlace = articulo.find('a')['href'] if articulo.find('a') else None  # Extraer el enlace\n",
    "        \n",
    "        if enlace:\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': 'https://www.eluniverso.com' + enlace  # Concatenar la URL base\n",
    "            })\n",
    "else:\n",
    "    print(f\"Error al acceder a la página principal. Código de estado: {response.status_code}\")\n",
    "\n",
    "# Convertir las noticias extraídas en un DataFrame de pandas\n",
    "df_noticias = pd.DataFrame(noticias)\n",
    "\n",
    "# Guardar el DataFrame en un archivo JSON\n",
    "df_noticias.to_json('noticias_eluniverso_principal.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "print(\"Datos guardados en 'noticias_eluniverso_principal.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista para almacenar las noticias\n",
    "noticias = []\n",
    "\n",
    "# Buscar todos los artículos de noticias en el HTML\n",
    "for item in soup.find_all('div', class_='catItemView groupLeading'):\n",
    "    titulo_element = item.find('h1', class_='story-heading')\n",
    "    if titulo_element:\n",
    "        titulo = titulo_element.get_text(strip=True)\n",
    "        enlace = titulo_element.find('a').get('href')\n",
    "        if enlace and enlace.startswith('/'):\n",
    "            enlace = f\"https://www.eltelegrafo.com.ec{enlace}\"\n",
    "        noticias.append({'titulo': titulo, 'enlace': enlace})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados en 'noticias_ecuador.json' con 265 entradas.\n"
     ]
    }
   ],
   "source": [
    "# Verificar que la solicitud fue exitosa\n",
    "url = 'https://cnnespanol.cnn.com/'  # Página a scrapear\n",
    "\n",
    "# Realizar la petición HTTP\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Buscar todos los enlaces con texto visible\n",
    "    noticias = soup.find_all('a', href=True)\n",
    "\n",
    "    titulos = []\n",
    "    enlaces = []\n",
    "\n",
    "    for noticia in noticias:\n",
    "        texto = noticia.get_text(strip=True)\n",
    "        href = noticia['href']\n",
    "\n",
    "        # Filtrar para evitar enlaces vacíos o muy cortos\n",
    "        if texto and len(texto) > 10:\n",
    "            # Construir URL absoluta si es relativa\n",
    "            if href.startswith('/'):\n",
    "                href = 'https://cnnespanol.cnn.com' + href\n",
    "            \n",
    "            titulos.append(texto)\n",
    "            enlaces.append(href)\n",
    "\n",
    "    # Crear DataFrame\n",
    "    df = pd.DataFrame({'titulo': titulos, 'link': enlaces})\n",
    "\n",
    "    # Guardar a JSON (formato lineas, utf-8)\n",
    "    df.to_json('noticias_ecuador.json', orient='records', force_ascii=False, lines=True)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_ecuador.json' con {len(df)} entradas.\")\n",
    "else:\n",
    "    print(f\"Error al acceder a la página: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               titulo  \\\n",
      "0   \"Advertimos, pero no nos escucharon\": Raúl Chá...   \n",
      "1   John Reimberg critica liberación de sospechoso...   \n",
      "2   \"El sistema financiero ecuatoriano es estable\"...   \n",
      "3   Niños en maletero de bus: ANT anuncia sancione...   \n",
      "4   Frente por un Nuevo IESS demandó a Noboa: ¿Qué...   \n",
      "5   Juicio político: Solanda Goyes denuncia motiva...   \n",
      "6   Gobierno transfiere $26,2 millones a Solca par...   \n",
      "7   Implicaciones de la suspensión de verificación...   \n",
      "8   Protestas en Quito: Esta semana están programa...   \n",
      "9   “Deberían estar presos”: Aquiles Álvarez arrem...   \n",
      "10  El impacto del narcotráfico en El Empalme: una...   \n",
      "11  Violencia intrafamiliar en Guayaquil: Segura E...   \n",
      "12  Reclutamiento de agentes de ATM: entidad alert...   \n",
      "13  Álvarez pidió formalmente a la Asamblea ser re...   \n",
      "14  Multas de tránsito en la ATM: así se encuentra...   \n",
      "15  Clima en Guayaquil: ¿Cuánto tiempo más durará ...   \n",
      "16  ¿Quién era el ultramaratonista fallecido en el...   \n",
      "17  Falsos repartidores armados asaltan pizzería e...   \n",
      "18  Festival Quito Luz de América: Ampliarán horar...   \n",
      "19  Ciudadanía presenta propuesta de ley para fort...   \n",
      "20  Policía Nacional ofrece equinoterapia gratuita...   \n",
      "21  EXPRESO y EXTRA denuncian campaña con fake new...   \n",
      "22  Adiós al papel higiénico: esta nueva alternati...   \n",
      "23  Vogue anuncia su Open Casting 2025: ¿Cómo se p...   \n",
      "24  ¿Qué pasó con Wendy Guevara? Esto se sabe sobr...   \n",
      "25  Jóvenes en Acción Ecuador: Cómo preparar tu ho...   \n",
      "26  Consejos para actualizar los sistemas de factu...   \n",
      "27  Cooperativas vs. Bancos: diferencias fundament...   \n",
      "28  Milei formaliza veto al aumento de las pension...   \n",
      "29  ¿La inseguridad llegó al campo? Los agricultor...   \n",
      "30  Banco Pichincha enfrentará un proceso sanciona...   \n",
      "31  Esta farmacia planea abrir 250 locales más en ...   \n",
      "32  Jair Bolsonaro: Corte Suprema ordena arresto d...   \n",
      "33  Cinco años después de la explosión, Beirut sig...   \n",
      "34  Alerta meteorológica en Escocia y norte de Ing...   \n",
      "35  Cafeterías con animales, de tendencia viral en...   \n",
      "36  Fórmula Uno: ¿Lewis Hamilton no va más en la e...   \n",
      "37  Copa Davis Ecuador: Selectivo para escoger al ...   \n",
      "38  Moisés Caicedo, de vuelta con Chelsea: Prepara...   \n",
      "\n",
      "                                               enlace  \n",
      "0   https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "1   https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "2   https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "3   https://www.expreso.echttps://www.expreso.ec/p...  \n",
      "4   https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "5   https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "6   https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "7   https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "8   https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "9   https://www.expreso.echttps://www.expreso.ec/g...  \n",
      "10  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "11  https://www.expreso.echttps://www.expreso.ec/g...  \n",
      "12  https://www.expreso.echttps://www.expreso.ec/g...  \n",
      "13  https://www.expreso.echttps://www.expreso.ec/g...  \n",
      "14  https://www.expreso.echttps://www.expreso.ec/g...  \n",
      "15  https://www.expreso.echttps://www.expreso.ec/g...  \n",
      "16  https://www.expreso.echttps://www.expreso.ec/d...  \n",
      "17  https://www.expreso.echttps://www.expreso.ec/q...  \n",
      "18  https://www.expreso.echttps://www.expreso.ec/q...  \n",
      "19  https://www.expreso.echttps://www.expreso.ec/q...  \n",
      "20  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "21  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "22  https://www.expreso.echttps://www.expreso.ec/b...  \n",
      "23  https://www.expreso.echttps://www.expreso.ec/b...  \n",
      "24  https://www.expreso.echttps://www.expreso.ec/o...  \n",
      "25  https://www.expreso.echttps://www.expreso.ec/s...  \n",
      "26  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "27  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "28  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "29  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "30  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "31  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "32  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "33  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "34  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "35  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "36  https://www.expreso.echttps://www.expreso.ec/d...  \n",
      "37  https://www.expreso.echttps://www.expreso.ec/d...  \n",
      "38  https://www.expreso.echttps://www.expreso.ec/d...  \n",
      "Datos guardados en 'noticias_expreso.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página principal de Expreso\n",
    "url = 'https://www.expreso.ec'\n",
    "\n",
    "# Realizar la solicitud GET\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Crear una lista para almacenar las noticias\n",
    "noticias = []\n",
    "\n",
    "# Buscar los títulos y enlaces de las noticias dentro de <h2 class=\"c-article__title\">\n",
    "articulos = soup.find_all('h2', class_='c-article__title')\n",
    "\n",
    "# Iterar sobre los artículos encontrados y extraer el título y el enlace\n",
    "for articulo in articulos:\n",
    "    titulo = articulo.get_text(strip=True)\n",
    "    enlace = articulo.find('a')['href'] if articulo.find('a') else None\n",
    "    if enlace:  # Verifica si el enlace no es None\n",
    "        noticias.append({\n",
    "            'titulo': titulo,\n",
    "            'enlace': url + enlace  # Agrega la URL base al enlace relativo\n",
    "        })\n",
    "\n",
    "# Convertir la lista de noticias en un DataFrame de pandas\n",
    "df_noticias = pd.DataFrame(noticias)\n",
    "\n",
    "# Guardar los datos en un archivo JSON\n",
    "df_noticias.to_json('noticias_expreso.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "# Imprimir el DataFrame\n",
    "print(df_noticias)\n",
    "\n",
    "print(\"Datos guardados en 'noticias_expreso.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               titulo  \\\n",
      "0   \"Advertimos, pero no nos escucharon\": Raúl Chá...   \n",
      "1   John Reimberg critica liberación de sospechoso...   \n",
      "2   \"El sistema financiero ecuatoriano es estable\"...   \n",
      "3   Niños en maletero de bus: ANT anuncia sancione...   \n",
      "4   Frente por un Nuevo IESS demandó a Noboa: ¿Qué...   \n",
      "5   Juicio político: Solanda Goyes denuncia motiva...   \n",
      "6   Gobierno transfiere $26,2 millones a Solca par...   \n",
      "7   Implicaciones de la suspensión de verificación...   \n",
      "8   Protestas en Quito: Esta semana están programa...   \n",
      "9   “Deberían estar presos”: Aquiles Álvarez arrem...   \n",
      "10  El impacto del narcotráfico en El Empalme: una...   \n",
      "11  Violencia intrafamiliar en Guayaquil: Segura E...   \n",
      "12  Reclutamiento de agentes de ATM: entidad alert...   \n",
      "13  Álvarez pidió formalmente a la Asamblea ser re...   \n",
      "14  Multas de tránsito en la ATM: así se encuentra...   \n",
      "15  Clima en Guayaquil: ¿Cuánto tiempo más durará ...   \n",
      "16  ¿Quién era el ultramaratonista fallecido en el...   \n",
      "17  Falsos repartidores armados asaltan pizzería e...   \n",
      "18  Festival Quito Luz de América: Ampliarán horar...   \n",
      "19  Ciudadanía presenta propuesta de ley para fort...   \n",
      "20  Policía Nacional ofrece equinoterapia gratuita...   \n",
      "21  EXPRESO y EXTRA denuncian campaña con fake new...   \n",
      "22  Adiós al papel higiénico: esta nueva alternati...   \n",
      "23  Vogue anuncia su Open Casting 2025: ¿Cómo se p...   \n",
      "24  ¿Qué pasó con Wendy Guevara? Esto se sabe sobr...   \n",
      "25  Jóvenes en Acción Ecuador: Cómo preparar tu ho...   \n",
      "26  Consejos para actualizar los sistemas de factu...   \n",
      "27  Cooperativas vs. Bancos: diferencias fundament...   \n",
      "28  Milei formaliza veto al aumento de las pension...   \n",
      "29  ¿La inseguridad llegó al campo? Los agricultor...   \n",
      "30  Banco Pichincha enfrentará un proceso sanciona...   \n",
      "31  Esta farmacia planea abrir 250 locales más en ...   \n",
      "32  Jair Bolsonaro: Corte Suprema ordena arresto d...   \n",
      "33  Cinco años después de la explosión, Beirut sig...   \n",
      "34  Alerta meteorológica en Escocia y norte de Ing...   \n",
      "35  Cafeterías con animales, de tendencia viral en...   \n",
      "36  Fórmula Uno: ¿Lewis Hamilton no va más en la e...   \n",
      "37  Copa Davis Ecuador: Selectivo para escoger al ...   \n",
      "38  Moisés Caicedo, de vuelta con Chelsea: Prepara...   \n",
      "\n",
      "                                               enlace  \n",
      "0   https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "1   https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "2   https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "3   https://www.expreso.echttps://www.expreso.ec/p...  \n",
      "4   https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "5   https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "6   https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "7   https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "8   https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "9   https://www.expreso.echttps://www.expreso.ec/g...  \n",
      "10  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "11  https://www.expreso.echttps://www.expreso.ec/g...  \n",
      "12  https://www.expreso.echttps://www.expreso.ec/g...  \n",
      "13  https://www.expreso.echttps://www.expreso.ec/g...  \n",
      "14  https://www.expreso.echttps://www.expreso.ec/g...  \n",
      "15  https://www.expreso.echttps://www.expreso.ec/g...  \n",
      "16  https://www.expreso.echttps://www.expreso.ec/d...  \n",
      "17  https://www.expreso.echttps://www.expreso.ec/q...  \n",
      "18  https://www.expreso.echttps://www.expreso.ec/q...  \n",
      "19  https://www.expreso.echttps://www.expreso.ec/q...  \n",
      "20  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "21  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "22  https://www.expreso.echttps://www.expreso.ec/b...  \n",
      "23  https://www.expreso.echttps://www.expreso.ec/b...  \n",
      "24  https://www.expreso.echttps://www.expreso.ec/o...  \n",
      "25  https://www.expreso.echttps://www.expreso.ec/s...  \n",
      "26  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "27  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "28  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "29  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "30  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "31  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "32  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "33  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "34  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "35  https://www.expreso.echttps://www.expreso.ec/a...  \n",
      "36  https://www.expreso.echttps://www.expreso.ec/d...  \n",
      "37  https://www.expreso.echttps://www.expreso.ec/d...  \n",
      "38  https://www.expreso.echttps://www.expreso.ec/d...  \n",
      "Datos guardados en 'noticias_expreso.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página principal de Expreso\n",
    "url = 'https://www.expreso.ec'\n",
    "\n",
    "# Realizar la solicitud GET\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Crear una lista para almacenar las noticias\n",
    "noticias = []\n",
    "\n",
    "# Buscar los títulos y enlaces de las noticias dentro de <h2 class=\"c-article__title\">\n",
    "articulos = soup.find_all('h2', class_='c-article__title')\n",
    "\n",
    "# Iterar sobre los artículos encontrados y extraer el título y el enlace\n",
    "for articulo in articulos:\n",
    "    titulo = articulo.get_text(strip=True)\n",
    "    enlace = articulo.find('a')['href'] if articulo.find('a') else None\n",
    "    if enlace:  # Verifica si el enlace no es None\n",
    "        noticias.append({\n",
    "            'titulo': titulo,\n",
    "            'enlace': url + enlace  # Agrega la URL base al enlace relativo\n",
    "        })\n",
    "\n",
    "# Convertir la lista de noticias en un DataFrame de pandas\n",
    "df_noticias = pd.DataFrame(noticias)\n",
    "\n",
    "# Guardar los datos en un archivo JSON\n",
    "df_noticias.to_json('noticias_expreso.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "# Imprimir el DataFrame\n",
    "print(df_noticias)\n",
    "\n",
    "print(\"Datos guardados en 'noticias_expreso.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceso exitoso a la página.\n",
      "Datos guardados en 'noticias_radio_sucre.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de Radio Sucre\n",
    "url = \"https://radiosucre.com.ec\"\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(\"Acceso exitoso a la página.\")\n",
    "    \n",
    "    # Analizar el contenido con BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Encontrar los títulos de las noticias\n",
    "    articulos = soup.find_all('h4', class_='title')\n",
    "    \n",
    "    # Lista para almacenar las noticias\n",
    "    noticias = []\n",
    "    \n",
    "    # Extraer los títulos y enlaces\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)\n",
    "        enlace = articulo.find('a')['href']\n",
    "        \n",
    "        noticias.append({\n",
    "            'titulo': titulo,\n",
    "            'enlace': enlace\n",
    "        })\n",
    "    \n",
    "    # Convertir a DataFrame de Pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "    \n",
    "    # Guardar como archivo JSON\n",
    "    df.to_json('noticias_radio_sucre.json', orient='records', lines=True, force_ascii=False)\n",
    "    \n",
    "    print(\"Datos guardados en 'noticias_radio_sucre.json'\")\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceso exitoso a la página.\n",
      "Datos guardados en 'noticias_notimundo.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de Notimundo\n",
    "url = \"https://notimundo.com.ec\"\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(\"Acceso exitoso a la página.\")\n",
    "    \n",
    "    # Analizar el contenido con BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Encontrar los títulos de las noticias\n",
    "    articulos = soup.find_all('h3', class_='entry-title td-module-title')\n",
    "    \n",
    "    # Lista para almacenar las noticias\n",
    "    noticias = []\n",
    "    \n",
    "    # Extraer los títulos y enlaces\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)\n",
    "        enlace = articulo.find('a')['href']\n",
    "        \n",
    "        noticias.append({\n",
    "            'titulo': titulo,\n",
    "            'enlace': enlace\n",
    "        })\n",
    "    \n",
    "    # Convertir a DataFrame de Pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "    \n",
    "    # Guardar como archivo JSON\n",
    "    df.to_json('noticias_notimundo.json', orient='records', lines=True, force_ascii=False)\n",
    "    \n",
    "    print(\"Datos guardados en 'noticias_notimundo.json'\")\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.vistazo.com/\n",
      "Datos guardados en 'noticias_vistazo.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página de Vistazo\n",
    "url = 'https://www.vistazo.com/'\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <h2>\n",
    "    articulos = soup.find_all('h2')\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)\n",
    "        enlace = articulo.find_parent('a')['href'] if articulo.find_parent('a') else None\n",
    "        \n",
    "        # Añadir la noticia a la lista\n",
    "        if enlace:\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': 'https://www.vistazo.com' + enlace  # Añadir el dominio al enlace\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_vistazo.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_vistazo.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.extra.ec/\n",
      "Datos guardados en 'noticias_extra.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página de Extra\n",
    "url = 'https://www.extra.ec/'\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <h2> con la clase 'c-article__title'\n",
    "    articulos = soup.find_all('h2', class_='c-article__title')\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)\n",
    "        enlace = articulo.find('a')['href'] if articulo.find('a') else None\n",
    "        \n",
    "        # Añadir la noticia a la lista\n",
    "        if enlace:\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': 'https://www.extra.ec' + enlace  # Añadir el dominio al enlace\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_extra.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_extra.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.hoy.com.ec/\n",
      "Datos guardados en 'noticias_hoy.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página de Hoy\n",
    "url = 'https://www.hoy.com.ec/'\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <h1> con la clase 'blog-entry-title entry-title'\n",
    "    articulos = soup.find_all('h1', class_='blog-entry-title entry-title')\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)\n",
    "        enlace = articulo.find('a')['href'] if articulo.find('a') else None\n",
    "        \n",
    "        # Añadir la noticia a la lista\n",
    "        if enlace:\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': 'https://www.hoy.com.ec' + enlace  # Añadir el dominio al enlace\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_hoy.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_hoy.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.larepublica.ec/\n",
      "Datos guardados en 'noticias_la_republica.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página de La República\n",
    "url = 'https://www.larepublica.ec/'\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <h3> con la clase 'mh-custom-posts-xl-title'\n",
    "    articulos = soup.find_all('h3', class_='mh-custom-posts-xl-title')\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)\n",
    "        enlace = articulo.find('a')['href'] if articulo.find('a') else None\n",
    "        \n",
    "        # Añadir la noticia a la lista\n",
    "        if enlace:\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': enlace  # El enlace ya está completo\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_la_republica.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_la_republica.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.planv.com.ec\n",
      "Datos guardados en 'noticias_planv.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página de PlanV\n",
    "url = 'https://www.planv.com.ec'\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <h3> con la clase 'entry-title td-module-title'\n",
    "    articulos = soup.find_all('h3', class_='entry-title td-module-title')\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)\n",
    "        enlace = articulo.find('a')['href'] if articulo.find('a') else None\n",
    "        \n",
    "        # Añadir la noticia a la lista\n",
    "        if enlace:\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': enlace  # El enlace ya está completo\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_planv.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_planv.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.ecuadortimes.net/\n",
      "Datos guardados en 'noticias_ecuador_times.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página de Ecuador Times\n",
    "url = 'https://www.ecuadortimes.net/'\n",
    "\n",
    "# Encabezados para simular un navegador real\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <h2> con la clase 'blog-title'\n",
    "    articulos = soup.find_all('h2', class_='blog-title')\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        enlace = articulo.find('a')  # Buscar el enlace dentro del h2\n",
    "        if enlace:\n",
    "            titulo = enlace.get_text(strip=True)\n",
    "            url_noticia = enlace['href']  # Obtener el atributo href del enlace\n",
    "            \n",
    "            # Añadir la noticia a la lista\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': url_noticia\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_ecuador_times.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_ecuador_times.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.bbc.com/news\n",
      "Datos guardados en 'noticias_bbc.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página de BBC News\n",
    "url = 'https://www.bbc.com/news'\n",
    "\n",
    "# Encabezados para evitar restricciones\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <h2> con el atributo data-testid=\"card-headline\"\n",
    "    articulos = soup.find_all('h2', {'data-testid': 'card-headline'})\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)\n",
    "        enlace = articulo.find_parent('a')['href'] if articulo.find_parent('a') else None\n",
    "        \n",
    "        # Añadir la noticia a la lista\n",
    "        if enlace:\n",
    "            # Asegurar que el enlace sea completo\n",
    "            if enlace.startswith('/'):\n",
    "                enlace = f\"https://www.bbc.com{enlace}\"\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': enlace\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_bbc.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_bbc.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.cnn.com/\n",
      "Datos guardados en 'noticias_cnn.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página principal de CNN\n",
    "url = 'https://www.cnn.com/'\n",
    "\n",
    "# Encabezados para evitar restricciones\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <h2> con la clase específica\n",
    "    articulos = soup.find_all('h2', class_='container__title_url-text container_lead-package__title_url-text')\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get_text(strip=True)\n",
    "        enlace = articulo.find_parent('a')['href'] if articulo.find_parent('a') else None\n",
    "        \n",
    "        # Añadir la noticia a la lista\n",
    "        if enlace:\n",
    "            # Asegurar que el enlace sea completo\n",
    "            if enlace.startswith('/'):\n",
    "                enlace = f\"https://www.cnn.com{enlace}\"\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': enlace\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_cnn.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_cnn.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.theguardian.com/\n",
      "Datos guardados en 'noticias_the_guardian.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página principal de The Guardian\n",
    "url = 'https://www.theguardian.com/'\n",
    "\n",
    "# Encabezados para evitar restricciones\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <a> con el atributo 'data-link-name'\n",
    "    articulos = soup.find_all('a', attrs={'data-link-name': lambda x: x and 'card' in x})\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        titulo = articulo.get('aria-label', 'Sin título').strip()\n",
    "        enlace = articulo['href'] if articulo.has_attr('href') else None\n",
    "        \n",
    "        # Añadir la noticia a la lista\n",
    "        if enlace:\n",
    "            # Asegurar que el enlace sea completo\n",
    "            if enlace.startswith('/'):\n",
    "                enlace = f\"https://www.theguardian.com{enlace}\"\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': enlace\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_the_guardian.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_the_guardian.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accediendo a la página: https://www.aljazeera.com/\n",
      "Datos guardados en 'noticias_aljazeera.json'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página principal de Al Jazeera\n",
    "url = 'https://www.aljazeera.com/'\n",
    "\n",
    "# Encabezados para evitar restricciones\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Accediendo a la página: {url}\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los títulos de las noticias dentro de las etiquetas <h3> con la clase 'gc__title'\n",
    "    articulos = soup.find_all('h3', class_='gc__title')\n",
    "    \n",
    "    # Crear una lista para almacenar las noticias\n",
    "    noticias = []\n",
    "\n",
    "    # Extraer título y enlace de cada noticia\n",
    "    for articulo in articulos:\n",
    "        enlace_element = articulo.find('a', class_='u-clickable-card__link')\n",
    "        titulo = enlace_element.find('span').get_text(strip=True) if enlace_element else \"Sin título\"\n",
    "        enlace = enlace_element['href'] if enlace_element and enlace_element.has_attr('href') else None\n",
    "        \n",
    "        # Añadir la noticia a la lista\n",
    "        if enlace:\n",
    "            # Asegurar que el enlace sea completo\n",
    "            if enlace.startswith('/'):\n",
    "                enlace = f\"https://www.aljazeera.com{enlace}\"\n",
    "            noticias.append({\n",
    "                'titulo': titulo,\n",
    "                'enlace': enlace\n",
    "            })\n",
    "    \n",
    "    # Convertir los datos a un DataFrame de pandas\n",
    "    df = pd.DataFrame(noticias)\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    df.to_json('noticias_aljazeera.json', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "    print(f\"Datos guardados en 'noticias_aljazeera.json'\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error al acceder a la página. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido correctamente:\n",
      "C:\\jupyter\\noticias_ecuador.json -> C:\\jupyter\\csv_files\\noticias_ecuador.csv\n"
     ]
    }
   ],
   "source": [
    "# Directorio donde están los archivos JSON\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Ruta del archivo JSON\n",
    "ruta_json = r'C:\\jupyter\\noticias_ecuador.json'\n",
    "\n",
    "# Carpeta donde guardarás los CSV\n",
    "directorio_csv = r'C:\\jupyter\\csv_files'\n",
    "\n",
    "# Crear la carpeta si no existe\n",
    "os.makedirs(directorio_csv, exist_ok=True)\n",
    "\n",
    "# Nombre del archivo CSV\n",
    "nombre_csv = 'noticias_ecuador.csv'\n",
    "ruta_csv = os.path.join(directorio_csv, nombre_csv)\n",
    "\n",
    "# Verificar que el archivo JSON existe\n",
    "if os.path.exists(ruta_json):\n",
    "    # Leer JSON como DataFrame\n",
    "    df = pd.read_json(ruta_json, lines=True)\n",
    "    \n",
    "    # Guardar como CSV\n",
    "    df.to_csv(ruta_csv, index=False, encoding='utf-8')\n",
    "    print(f\"Archivo convertido correctamente:\\n{ruta_json} -> {ruta_csv}\")\n",
    "else:\n",
    "    print(f\"⚠️ Archivo no encontrado: {ruta_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido correctamente:\n",
      "C:\\jupyter\\noticias_ecuador.json -> C:\\jupyter\\csv_files\\noticias_ecuador.csv\n"
     ]
    }
   ],
   "source": [
    "# Subir el archivo CSV a Couchbase \n",
    "# Ruta del archivo JSON original\n",
    "ruta_json = r'C:\\jupyter\\noticias_ecuador.json'\n",
    "\n",
    "# Carpeta donde guardarás los CSV\n",
    "directorio_csv = r'C:\\jupyter\\csv_files'\n",
    "\n",
    "# Crear la carpeta si no existe\n",
    "os.makedirs(directorio_csv, exist_ok=True)\n",
    "\n",
    "# Nombre del archivo CSV de salida\n",
    "nombre_csv = 'noticias_ecuador.csv'\n",
    "ruta_csv = os.path.join(directorio_csv, nombre_csv)\n",
    "\n",
    "# Verificar que el archivo JSON existe\n",
    "if os.path.exists(ruta_json):\n",
    "    # Leer JSON y convertir a DataFrame\n",
    "    df = pd.read_json(ruta_json, lines=True)\n",
    "    \n",
    "    # Guardar como CSV\n",
    "    df.to_csv(ruta_csv, index=False, encoding='utf-8')\n",
    "    print(f\"Archivo convertido correctamente:\\n{ruta_json} -> {ruta_csv}\")\n",
    "else:\n",
    "    print(f\"⚠️ Archivo no encontrado: {ruta_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los CSV se han combinado en 'noticias_ecuador.csv'\n"
     ]
    }
   ],
   "source": [
    "# Lista para almacenar los DataFrames\n",
    "csv_combinados = []\n",
    "\n",
    "# Leer todos los CSV en el directorio\n",
    "for archivo_csv in os.listdir(directorio_csv):\n",
    "    if archivo_csv.endswith('.csv'):\n",
    "        ruta_csv = os.path.join(directorio_csv, archivo_csv)\n",
    "        df = pd.read_csv(ruta_csv)\n",
    "        csv_combinados.append(df)\n",
    "\n",
    "# Combinar todos los DataFrames en uno solo\n",
    "df_combinado = pd.concat(csv_combinados, ignore_index=True)\n",
    "\n",
    "# Guardar el archivo combinado como CSV\n",
    "df_combinado.to_csv('noticias_ecuador.csv', index=False, encoding='utf-8')\n",
    "print(\"Todos los CSV se han combinado en 'noticias_ecuador.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV content inserted into Couchbase.\n"
     ]
    }
   ],
   "source": [
    "# Subir el archivo CSV a Couchbase \n",
    "try:\n",
    "    with open('noticias_ecuador.csv', 'r', encoding='utf-8') as f:\n",
    "        csv_content = f.read()\n",
    "    \n",
    "    cb_coll.insert('noticias_ecuador_csv', {'content': csv_content})\n",
    "    print(\"CSV content inserted into Couchbase.\")\n",
    "except CouchbaseException as e:\n",
    "    print(f\"Error inserting CSV content into Couchbase: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"General error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
